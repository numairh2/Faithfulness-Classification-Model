{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GSM8K Chain-of-Thought Generation (Colab Optimized)\n\nThis notebook generates chain-of-thought reasoning for GSM8K math problems using language models.\n**Optimized for Google Colab with GPU acceleration.**\n\n‚ö†Ô∏è **Important**: Make sure to enable GPU runtime in Colab:\n- Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí **GPU (T4)**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Colab Environment Setup (Run First!)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages in Colab\n!pip install -q transformers datasets accelerate bitsandbytes\n\n# Import libraries\nimport os\nimport json\nimport torch\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm.auto import tqdm\nimport gc\nimport time\nimport psutil\n\nprint(\"üì¶ Packages installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Verify Colab GPU Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nMODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\nOUTPUT_DIR = \"./cot_output\"\nMAX_PROBLEMS = 200\n\n# Critical: Verify GPU is available\nprint(\"üîç Checking Colab environment...\")\nprint(f\"PyTorch version: {torch.__version__}\")\n\nif torch.cuda.is_available():\n    DEVICE = \"cuda\"\n    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name()}\")\n    print(f\"   CUDA version: {torch.version.cuda}\")\n    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    print(f\"   Memory free: {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB\")\nelse:\n    DEVICE = \"cpu\"\n    print(\"‚ùå No GPU detected! This will be VERY slow (2+ minutes per sample)\")\n    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n\n# Check system memory\nram_gb = psutil.virtual_memory().total / 1024**3\nprint(f\"üíæ System RAM: {ram_gb:.1f} GB\")\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# HuggingFace token (optional for public models)\nHF_TOKEN = os.getenv(\"HF_TOKEN\")\nif HF_TOKEN:\n    print(\"‚úÖ HF_TOKEN found\")\nelse:\n    print(\"‚ÑπÔ∏è  No HF_TOKEN (OK for public models)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load and Explore GSM8K Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load GSM8K dataset\nprint(\"üìä Loading GSM8K dataset...\")\ndataset = load_dataset(\"gsm8k\", \"main\")\ntrain_data = dataset[\"train\"]\n\nprint(f\"Total problems in GSM8K train: {len(train_data)}\")\nprint(f\"Will process: {min(MAX_PROBLEMS, len(train_data))} problems\")\n\n# Show sample problem\nsample = train_data[0]\nprint(\"\\n=== Sample Problem ===\")\nprint(f\"Question: {sample['question']}\")\nprint(f\"Answer: {sample['answer']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 3. Load Model (Colab Optimized)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Clear any existing cache\nif DEVICE == \"cuda\":\n    torch.cuda.empty_cache()\ngc.collect()\n\nprint(\"üîß Loading tokenizer...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n    \n    # Fix tokenizer threading conflicts\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        print(\"‚úÖ Set pad_token to eos_token\")\n    \n    print(f\"‚úÖ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Error loading tokenizer: {e}\")\n    raise\n\nprint(\"üöÄ Loading model with Colab optimizations...\")\nstart_time = time.time()\n\ntry:\n    if DEVICE == \"cuda\":\n        # Colab GPU optimizations\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            torch_dtype=torch.float16,      # Essential for GPU performance\n            device_map=\"auto\",              # Let transformers handle device placement\n            load_in_8bit=True,              # Reduce memory usage (requires bitsandbytes)\n            trust_remote_code=True,\n            token=HF_TOKEN\n        )\n    else:\n        # CPU fallback\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            torch_dtype=torch.float32,\n            token=HF_TOKEN\n        ).to(DEVICE)\n    \n    model.eval()\n    load_time = time.time() - start_time\n    \n    print(f\"‚úÖ Model loaded in {load_time:.1f}s\")\n    print(f\"Model device: {next(model.parameters()).device}\")\n    \n    # Memory usage\n    if DEVICE == \"cuda\":\n        allocated = torch.cuda.memory_allocated() / 1024**3\n        reserved = torch.cuda.memory_reserved() / 1024**3\n        print(f\"GPU memory allocated: {allocated:.2f} GB\")\n        print(f\"GPU memory reserved: {reserved:.2f} GB\")\n        \n        # Expected performance\n        print(\"\\n‚ö° Expected performance on GPU: 15-30 seconds per sample\")\n    else:\n        print(\"\\nüêå CPU mode: 2+ minutes per sample (very slow!)\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error loading model: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"1. Make sure GPU runtime is enabled\")\n    print(\"2. Try restarting runtime if out of memory\")\n    print(\"3. Consider using a smaller model\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cot(problem, max_retries=3):\n",
    "    \"\"\"Generate chain-of-thought reasoning for a math problem.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a math reasoning assistant. \n",
    "Solve the following problem using step-by-step chain-of-thought reasoning and give the final answer at the end.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Answer with:\n",
    "<reasoning>\n",
    "...step-by-step reasoning...\n",
    "</reasoning>\n",
    "<final>\n",
    "...final numeric answer...\n",
    "</final>\n",
    "\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Clear cache before each generation\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=1024\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=400,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.4,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode\n",
    "            text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in 2 seconds...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(f\"All attempts failed for problem\")\n",
    "                raise e\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ CoT generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def generate_cot(problem, max_retries=3):\n    \"\"\"Generate chain-of-thought reasoning with Colab optimizations.\"\"\"\n    \n    prompt = f\"\"\"You are a math reasoning assistant. \nSolve the following problem using step-by-step chain-of-thought reasoning and give the final answer at the end.\n\nProblem:\n{problem}\n\nAnswer with:\n<reasoning>\n...step-by-step reasoning...\n</reasoning>\n<final>\n...final numeric answer...\n</final>\n\"\"\"\n    \n    for attempt in range(max_retries):\n        try:\n            # Colab memory management\n            if DEVICE == \"cuda\":\n                torch.cuda.empty_cache()\n            gc.collect()\n            \n            # Optimized tokenization\n            inputs = tokenizer(\n                prompt, \n                return_tensors=\"pt\", \n                padding=True, \n                truncation=True, \n                max_length=1024\n            )\n            \n            # Move to device (handle device_map=\"auto\")\n            if hasattr(model, 'device'):\n                device = model.device\n            else:\n                device = next(model.parameters()).device\n            \n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            \n            # Optimized generation for Colab\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=300,         # Reduced for speed\n                    do_sample=True,\n                    temperature=0.7,            # Higher for faster generation\n                    top_p=0.9,\n                    pad_token_id=tokenizer.eos_token_id,\n                    use_cache=True,             # Enable KV cache\n                    repetition_penalty=1.1      # Prevent repetition\n                )\n            \n            # Decode result\n            text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n            # Clean up tensors\n            del inputs, outputs\n            if DEVICE == \"cuda\":\n                torch.cuda.empty_cache()\n            \n            return text\n            \n        except Exception as e:\n            print(f\"‚ùå Attempt {attempt + 1} failed: {e}\")\n            # Clean up on error\n            if DEVICE == \"cuda\":\n                torch.cuda.empty_cache()\n            gc.collect()\n            \n            if attempt < max_retries - 1:\n                print(f\"Retrying in 2 seconds...\")\n                time.sleep(2)\n            else:\n                print(f\"All attempts failed for problem\")\n                raise e\n    \n    return None\n\nprint(\"‚úÖ Colab-optimized CoT generation function defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first problem\n",
    "print(\"Testing with sample problem...\")\n",
    "test_problem = train_data[0]['question']\n",
    "\n",
    "print(f\"Problem: {test_problem}\")\n",
    "print(\"\\nGenerating CoT...\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    result = generate_cot(test_problem)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"‚úÖ Generation successful! Time: {end_time - start_time:.2f}s\")\n",
    "    print(\"\\n=== Generated CoT ===\")\n",
    "    print(result)\n",
    "    \n",
    "    # Check memory usage\n",
    "    if DEVICE == \"cuda\":\n",
    "        print(f\"\\nGPU memory after generation: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")\n",
    "    print(\"This is likely where your mutex error occurs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Performance test with timing\nprint(\"üß™ Performance Test - Single Example\")\ntest_problem = train_data[0]['question']\n\nprint(f\"Problem: {test_problem}\")\nprint(f\"Expected answer: {train_data[0]['answer']}\")\nprint(\"\\n‚è±Ô∏è  Generating CoT...\")\n\n# Warm up GPU (first generation is always slower)\nif DEVICE == \"cuda\":\n    print(\"üî• GPU warmup...\")\n    _ = tokenizer(\"Test\", return_tensors=\"pt\")\n\nstart_time = time.time()\ntry:\n    result = generate_cot(test_problem)\n    end_time = time.time()\n    generation_time = end_time - start_time\n    \n    print(f\"‚úÖ Generation successful!\")\n    print(f\"‚è±Ô∏è  Time: {generation_time:.1f} seconds\")\n    \n    # Performance assessment\n    if generation_time < 30:\n        print(\"üöÄ Excellent! Good GPU performance\")\n    elif generation_time < 60:\n        print(\"‚ö° Good performance\")\n    elif generation_time < 120:\n        print(\"üêå Slow - check if GPU is being used\")\n    else:\n        print(\"üö® Very slow! Likely running on CPU\")\n        print(\"   Check GPU runtime settings\")\n    \n    print(\"\\n=== Generated CoT (first 500 chars) ===\")\n    print(result[:500] + \"...\" if len(result) > 500 else result)\n    \n    # Memory info\n    if DEVICE == \"cuda\":\n        allocated = torch.cuda.memory_allocated() / 1024**3\n        print(f\"\\nüíæ GPU memory after test: {allocated:.2f} GB\")\n        \n        # Estimate time for full dataset\n        total_time_hours = (generation_time * MAX_PROBLEMS) / 3600\n        print(f\"üìä Estimated time for {MAX_PROBLEMS} problems: {total_time_hours:.1f} hours\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Test failed: {e}\")\n    print(\"\\nüîß Troubleshooting:\")\n    print(\"1. Restart Colab runtime\")\n    print(\"2. Verify GPU is enabled\")\n    print(\"3. Check if model is too large for available memory\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process problems in batches\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "# Determine range to process\n",
    "num_to_process = min(MAX_PROBLEMS, len(train_data))\n",
    "\n",
    "print(f\"Starting batch processing of {num_to_process} problems...\")\n",
    "\n",
    "for i in tqdm(range(num_to_process), desc=\"Generating CoTs\"):\n",
    "    item = train_data[i]\n",
    "    problem_text = item['question']\n",
    "    gold_answer = item['answer']\n",
    "    \n",
    "    # Check if already processed\n",
    "    output_file = f\"{OUTPUT_DIR}/gsm8k_{i}.json\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Skipping {i} (already exists)\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Generate CoT\n",
    "        generated_cot = generate_cot(problem_text)\n",
    "        \n",
    "        # Save individual result\n",
    "        result = {\n",
    "            \"id\": i,\n",
    "            \"problem\": problem_text,\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"generated_cot\": generated_cot\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Print progress every 10 problems\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Completed {i + 1}/{num_to_process} problems\")\n",
    "            if DEVICE == \"cuda\":\n",
    "                print(f\"GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_info = {\n",
    "            \"id\": i,\n",
    "            \"problem\": problem_text,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        errors.append(error_info)\n",
    "        print(f\"‚ùå Error on problem {i}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Small delay to prevent overwhelming the system\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete!\")\n",
    "print(f\"Successful: {len(results)}\")\n",
    "print(f\"Errors: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all generated results\n",
    "all_results = []\n",
    "\n",
    "# Collect all JSON files from output directory\n",
    "for filename in os.listdir(OUTPUT_DIR):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(OUTPUT_DIR, filename), 'r') as f:\n",
    "            all_results.append(json.load(f))\n",
    "\n",
    "print(f\"Found {len(all_results)} generated CoTs\")\n",
    "\n",
    "if all_results:\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"\\n=== Dataset Summary ===\")\n",
    "    print(f\"Total problems: {len(df)}\")\n",
    "    \n",
    "    # Analyze CoT lengths\n",
    "    df['cot_length'] = df['generated_cot'].apply(len)\n",
    "    print(f\"Average CoT length: {df['cot_length'].mean():.0f} characters\")\n",
    "    print(f\"Min CoT length: {df['cot_length'].min()}\")\n",
    "    print(f\"Max CoT length: {df['cot_length'].max()}\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(\"\\n=== Sample Results ===\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        row = df.iloc[i]\n",
    "        print(f\"\\nProblem {row['id']}:\")\n",
    "        print(f\"Question: {row['problem'][:100]}...\")\n",
    "        print(f\"Generated CoT: {row['generated_cot'][:200]}...\")\n",
    "else:\n",
    "    print(\"No results found. Check for errors in generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export for FCM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSONL format for FCM training\n",
    "if all_results:\n",
    "    output_jsonl = \"data_processed/gsm8k_cots_notebook.jsonl\"\n",
    "    os.makedirs(\"data_processed\", exist_ok=True)\n",
    "    \n",
    "    with open(output_jsonl, 'w') as f:\n",
    "        for result in all_results:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "    \n",
    "    print(f\"‚úÖ Exported {len(all_results)} results to {output_jsonl}\")\n",
    "    \n",
    "    # Also create summary statistics\n",
    "    summary = {\n",
    "        \"total_problems\": len(all_results),\n",
    "        \"generation_model\": MODEL_NAME,\n",
    "        \"device_used\": DEVICE,\n",
    "        \"average_cot_length\": df['cot_length'].mean() if 'df' in locals() else 0,\n",
    "        \"errors_encountered\": len(errors) if 'errors' in locals() else 0\n",
    "    }\n",
    "    \n",
    "    with open(\"data_processed/generation_summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Summary saved to data_processed/generation_summary.json\")\n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Memory cleanup complete\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running this notebook successfully:\n",
    "\n",
    "1. **Check Results**: Verify the generated CoTs in `data_processed/gsm8k_cots_notebook.jsonl`\n",
    "2. **Data Pipeline**: Continue with answer extraction and faithfulness labeling\n",
    "3. **FCM Training**: Use the generated data for Faithfulness Classification Model training\n",
    "\n",
    "If you encounter the mutex error:\n",
    "- Check which cell it occurs in\n",
    "- Try running cells individually\n",
    "- Consider using a smaller model or CPU-only mode\n",
    "- Monitor memory usage throughout the process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}